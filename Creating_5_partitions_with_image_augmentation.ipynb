{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZxtfs5Jpaxx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import cv2\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMkAMd2XgQQh",
        "outputId": "740f74b1-b376-4d44-ac3d-b7e915216897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.6.4-py3-none-any.whl (803 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.1/803.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-cv) (23.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras-cv) (2023.6.3)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras-cv) (4.9.3)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (1.23.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (13.6.0)\n",
            "Collecting namex (from keras-core->keras-cv)\n",
            "  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (3.9.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras-cv) (0.1.8)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath,etree]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.5.2)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.31.0)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (4.66.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras-cv) (1.14.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (2023.6.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (6.1.1)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets->keras-cv) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets->keras-cv) (2023.7.22)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras-cv) (2.16.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.61.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Installing collected packages: namex, keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.6.4 namex-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install keras-cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyFNWQsMgUrL",
        "outputId": "fa283658-795b-44fd-f475-950553f29cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import keras_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zaww-78fpmJV",
        "outputId": "5d96d8c8-c4e7-4e80-e76e-4654b669f572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hpaVKy8pnMi"
      },
      "outputs": [],
      "source": [
        "origem_data_dir = '/content/drive/My Drive/Colab Notebooks/Mestrado final/RXT 224x224 mix_data/'\n",
        "trein_1234 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/trein 1-2-3-4/'\n",
        "trein_1235 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/trein 1-2-3-5/'\n",
        "trein_1245 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/trein 1-2-4-5/'\n",
        "trein_1345 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/trein 1-3-4-5/'\n",
        "trein_2345 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/trein 2-3-4-5/'\n",
        "part_1 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/1/'\n",
        "part_2 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/2/'\n",
        "part_3 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/3/'\n",
        "part_4 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/4/'\n",
        "part_5 = '/content/drive/My Drive/Colab Notebooks/Mestrado final/KFolds mix_data aumentado/5/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf3yEE4VpzaN",
        "outputId": "1a785469-f924-425d-963c-ca8f4398fc8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tuberculose', 'dip', 'covid', 'normais']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tf.io.gfile.listdir(origem_data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc0MBzLLm4gJ",
        "outputId": "1bfbecd5-fd40-46a5-c3cc-6dd300759156"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho normais :475\n",
            "Tamanho dip :262\n",
            "Tamanho covid :475\n",
            "Tamanho tuberculose :475\n"
          ]
        }
      ],
      "source": [
        "print('Tamanho normais :' + str(len(tf.io.gfile.listdir(origem_data_dir + 'normais'))))\n",
        "print('Tamanho dip :' + str(len(tf.io.gfile.listdir(origem_data_dir + 'dip'))))\n",
        "print('Tamanho covid :' + str(len(tf.io.gfile.listdir(origem_data_dir + 'covid'))))\n",
        "print('Tamanho tuberculose :' + str(len(tf.io.gfile.listdir(origem_data_dir + 'tuberculose'))))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Tamanho trein_1234 :' + str(len(tf.io.gfile.listdir(trein_1234 + 'tuberculose'))))\n",
        "print('Tamanho trein_1235 :' + str(len(tf.io.gfile.listdir(trein_1235 + 'tuberculose'))))\n",
        "print('Tamanho trein_1245 :' + str(len(tf.io.gfile.listdir(trein_1245 + 'tuberculose'))))\n",
        "print('Tamanho trein_1345 :' + str(len(tf.io.gfile.listdir(trein_1345 + 'tuberculose'))))\n",
        "print('Tamanho trein_2345 :' + str(len(tf.io.gfile.listdir(trein_2345 + 'tuberculose'))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFtscZ24YuDz",
        "outputId": "76d9d997-fd9a-4137-8832-e6eb40c3ac83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho trein_1234 :0\n",
            "Tamanho trein_1235 :0\n",
            "Tamanho trein_1245 :0\n",
            "Tamanho trein_1345 :0\n",
            "Tamanho trein_2345 :0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5K4oXg7UH3M"
      },
      "outputs": [],
      "source": [
        "def distribuir_imagens_particoes (nome_classe, num_part):\n",
        "\n",
        "  #pegando nomes imagens de uma classe\n",
        "  images_names = tf.io.gfile.listdir(origem_data_dir + nome_classe)\n",
        "  random.shuffle(images_names)\n",
        "\n",
        "  teste_size = 0\n",
        "  total_part_size = (len(images_names)-teste_size)\n",
        "  part_size = np.around(total_part_size/num_part).astype(int)\n",
        "\n",
        "  #copiando para partições (sem aumentar)\n",
        "  part1 = images_names[teste_size:(part_size+teste_size)]\n",
        "  part2 = images_names[(part_size+teste_size):(part_size*2+teste_size)]\n",
        "  part3 = images_names[(part_size*2+teste_size):(part_size*3+teste_size)]\n",
        "  part4 = images_names[(part_size*3+teste_size):(part_size*4+teste_size)]\n",
        "  part5 = images_names[(part_size*4+teste_size):]\n",
        "\n",
        "  for i in part1:\n",
        "    img = cv2.imread(origem_data_dir + nome_classe + '/' + i)\n",
        "    cv2.imwrite(part_1 + nome_classe + '/' + i, img)\n",
        "\n",
        "  for i in part2:\n",
        "    img = cv2.imread(origem_data_dir + nome_classe + '/' + i)\n",
        "    cv2.imwrite(part_2 + nome_classe + '/' + i, img)\n",
        "\n",
        "  for i in part3:\n",
        "    img = cv2.imread(origem_data_dir + nome_classe + '/' + i)\n",
        "    cv2.imwrite(part_3 + nome_classe + '/' + i, img)\n",
        "\n",
        "  for i in part4:\n",
        "    img = cv2.imread(origem_data_dir + nome_classe + '/' + i)\n",
        "    cv2.imwrite(part_4 + nome_classe + '/' + i, img)\n",
        "\n",
        "  for i in part5:\n",
        "    img = cv2.imread(origem_data_dir + nome_classe + '/' + i)\n",
        "    cv2.imwrite(part_5 + nome_classe + '/' + i, img)\n",
        "\n",
        "  print(nome_classe)\n",
        "  print('Tamanho origem :' + str(len(tf.io.gfile.listdir(origem_data_dir + nome_classe))))\n",
        "  print('Tamanho Folder1 :' + str(len(tf.io.gfile.listdir(part_1 + nome_classe))))\n",
        "  print('Tamanho Folder2 :' + str(len(tf.io.gfile.listdir(part_2 + nome_classe))))\n",
        "  print('Tamanho Folder3 :' + str(len(tf.io.gfile.listdir(part_3 + nome_classe))))\n",
        "  print('Tamanho Folder4 :' + str(len(tf.io.gfile.listdir(part_4 + nome_classe))))\n",
        "  print('Tamanho Folder5 :' + str(len(tf.io.gfile.listdir(part_5 + nome_classe))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iiRkWDKUH5-",
        "outputId": "3779ead1-c6bb-4c34-a272-0ee29e30ab6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid\n",
            "Tamanho origem :475\n",
            "Tamanho Folder1 :95\n",
            "Tamanho Folder2 :95\n",
            "Tamanho Folder3 :95\n",
            "Tamanho Folder4 :95\n",
            "Tamanho Folder5 :95\n"
          ]
        }
      ],
      "source": [
        "distribuir_imagens_particoes('covid', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L4DqYQ8bIT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bcf6909-82e8-438e-bb49-aea536e82138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dip\n",
            "Tamanho origem :262\n",
            "Tamanho Folder1 :52\n",
            "Tamanho Folder2 :52\n",
            "Tamanho Folder3 :52\n",
            "Tamanho Folder4 :52\n",
            "Tamanho Folder5 :54\n"
          ]
        }
      ],
      "source": [
        "distribuir_imagens_particoes('dip', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2RavrTdbInr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df41b1b3-09f9-4df2-ce14-0695bea72398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normais\n",
            "Tamanho origem :475\n",
            "Tamanho Folder1 :95\n",
            "Tamanho Folder2 :95\n",
            "Tamanho Folder3 :95\n",
            "Tamanho Folder4 :95\n",
            "Tamanho Folder5 :95\n"
          ]
        }
      ],
      "source": [
        "distribuir_imagens_particoes('normais', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGLllIRobI_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a953251-7757-4ae8-98ff-e6c4435ec82c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuberculose\n",
            "Tamanho origem :475\n",
            "Tamanho Folder1 :95\n",
            "Tamanho Folder2 :95\n",
            "Tamanho Folder3 :95\n",
            "Tamanho Folder4 :95\n",
            "Tamanho Folder5 :95\n"
          ]
        }
      ],
      "source": [
        "distribuir_imagens_particoes('tuberculose', 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES2gwIz1oN-8"
      },
      "outputs": [],
      "source": [
        "def mover_pastas_treinam (nome_classe):\n",
        "\n",
        "  part1_names = tf.io.gfile.listdir(part_1 + nome_classe)\n",
        "  part2_names = tf.io.gfile.listdir(part_2 + nome_classe)\n",
        "  part3_names = tf.io.gfile.listdir(part_3 + nome_classe)\n",
        "  part4_names = tf.io.gfile.listdir(part_4 + nome_classe)\n",
        "  part5_names = tf.io.gfile.listdir(part_5 + nome_classe)\n",
        "\n",
        "  #partição 5 de fora\n",
        "  for i in part1_names:\n",
        "    img = cv2.imread(part_1 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1234 + nome_classe + '/' + i, img)\n",
        "  for i in part2_names:\n",
        "    img = cv2.imread(part_2 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1234 + nome_classe + '/' + i, img)\n",
        "  for i in part3_names:\n",
        "    img = cv2.imread(part_3 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1234 + nome_classe + '/' + i, img)\n",
        "  for i in part4_names:\n",
        "    img = cv2.imread(part_4 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1234 + nome_classe + '/' + i, img)\n",
        "\n",
        "  #partição 4 de fora\n",
        "  for i in part1_names:\n",
        "    img = cv2.imread(part_1 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1235 + nome_classe + '/' + i, img)\n",
        "  for i in part2_names:\n",
        "    img = cv2.imread(part_2 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1235 + nome_classe + '/' + i, img)\n",
        "  for i in part3_names:\n",
        "    img = cv2.imread(part_3 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1235 + nome_classe + '/' + i, img)\n",
        "  for i in part5_names:\n",
        "    img = cv2.imread(part_5 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1235 + nome_classe + '/' + i, img)\n",
        "\n",
        "  #partição 3 de fora\n",
        "  for i in part1_names:\n",
        "    img = cv2.imread(part_1 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1245 + nome_classe + '/' + i, img)\n",
        "  for i in part2_names:\n",
        "    img = cv2.imread(part_2 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1245 + nome_classe + '/' + i, img)\n",
        "  for i in part5_names:\n",
        "    img = cv2.imread(part_5 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1245 + nome_classe + '/' + i, img)\n",
        "  for i in part4_names:\n",
        "    img = cv2.imread(part_4 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1245 + nome_classe + '/' + i, img)\n",
        "\n",
        "  #partição 2 de fora\n",
        "  for i in part1_names:\n",
        "    img = cv2.imread(part_1 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1345 + nome_classe + '/' + i, img)\n",
        "  for i in part5_names:\n",
        "    img = cv2.imread(part_5 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1345 + nome_classe + '/' + i, img)\n",
        "  for i in part3_names:\n",
        "    img = cv2.imread(part_3 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1345 + nome_classe + '/' + i, img)\n",
        "  for i in part4_names:\n",
        "    img = cv2.imread(part_4 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_1345 + nome_classe + '/' + i, img)\n",
        "\n",
        "  #partição 1 de fora\n",
        "  for i in part5_names:\n",
        "    img = cv2.imread(part_5 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_2345 + nome_classe + '/' + i, img)\n",
        "  for i in part2_names:\n",
        "    img = cv2.imread(part_2 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_2345 + nome_classe + '/' + i, img)\n",
        "  for i in part3_names:\n",
        "    img = cv2.imread(part_3 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_2345 + nome_classe + '/' + i, img)\n",
        "  for i in part4_names:\n",
        "    img = cv2.imread(part_4 + nome_classe + '/' + i)\n",
        "    cv2.imwrite(trein_2345 + nome_classe + '/' + i, img)\n",
        "\n",
        "\n",
        "  print(nome_classe)\n",
        "  print('Tamanho trein_1234:' + str(len(tf.io.gfile.listdir(trein_1234+nome_classe))))\n",
        "  print('Tamanho trein_1235:' + str(len(tf.io.gfile.listdir(trein_1235+nome_classe))))\n",
        "  print('Tamanho trein_1245:' + str(len(tf.io.gfile.listdir(trein_1245+nome_classe))))\n",
        "  print('Tamanho trein_1345:' + str(len(tf.io.gfile.listdir(trein_1345+nome_classe))))\n",
        "  print('Tamanho trein_2345:' + str(len(tf.io.gfile.listdir(trein_2345+nome_classe))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMiqD7bkoOBf",
        "outputId": "f85c2b39-4a36-4a07-d7f9-b4b661ed6793"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid\n",
            "Tamanho trein_1234:380\n",
            "Tamanho trein_1235:380\n",
            "Tamanho trein_1245:380\n",
            "Tamanho trein_1345:380\n",
            "Tamanho trein_2345:380\n"
          ]
        }
      ],
      "source": [
        "mover_pastas_treinam('covid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZSsDTAFoOEC",
        "outputId": "b24a5178-81ba-4c96-8faf-4a5d59aa4d32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dip\n",
            "Tamanho trein_1234:208\n",
            "Tamanho trein_1235:210\n",
            "Tamanho trein_1245:210\n",
            "Tamanho trein_1345:210\n",
            "Tamanho trein_2345:210\n"
          ]
        }
      ],
      "source": [
        "mover_pastas_treinam('dip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVbAFPfItPev",
        "outputId": "842cdd24-116b-41e9-ec8c-aac5c78db5f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normais\n",
            "Tamanho trein_1234:380\n",
            "Tamanho trein_1235:380\n",
            "Tamanho trein_1245:380\n",
            "Tamanho trein_1345:380\n",
            "Tamanho trein_2345:380\n"
          ]
        }
      ],
      "source": [
        "mover_pastas_treinam('normais')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAKkALQotPh7",
        "outputId": "465c74d8-718a-4a4e-f622-8089751d995a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuberculose\n",
            "Tamanho trein_1234:380\n",
            "Tamanho trein_1235:380\n",
            "Tamanho trein_1245:380\n",
            "Tamanho trein_1345:380\n",
            "Tamanho trein_2345:380\n"
          ]
        }
      ],
      "source": [
        "mover_pastas_treinam('tuberculose')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def aumentar_imagens (nome_classe, magnitude_x=0.2, augmentations_per_image_x=3, n_transformacoes=3):\n",
        "\n",
        "  trein_1234_names = tf.io.gfile.listdir(trein_1234 + nome_classe)\n",
        "  trein_1235_names = tf.io.gfile.listdir(trein_1235 + nome_classe)\n",
        "  trein_1245_names = tf.io.gfile.listdir(trein_1245 + nome_classe)\n",
        "  trein_1345_names = tf.io.gfile.listdir(trein_1345 + nome_classe)\n",
        "  trein_2345_names = tf.io.gfile.listdir(trein_2345 + nome_classe)\n",
        "\n",
        "  #partição 1\n",
        "  for i in trein_1234_names:    #manda a original\n",
        "    img = cv2.imread(trein_1234 + nome_classe + '/' + i)\n",
        "    for j in range(n_transformacoes):\n",
        "      aug_img = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=magnitude_x,magnitude_stddev = 0.15,augmentations_per_image=augmentations_per_image_x,rate = 1,geometric = True)(img)\n",
        "      aug_img_nump = aug_img.numpy().astype('int')\n",
        "      cv2.imwrite(trein_1234 + nome_classe + '/' + 't' + str(j) + '_' + i, aug_img_nump)\n",
        "\n",
        "  #partição 2\n",
        "  for i in trein_1235_names:    #manda a original\n",
        "    img = cv2.imread(trein_1235 + nome_classe + '/' + i)\n",
        "    for j in range(n_transformacoes):\n",
        "      aug_img = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=magnitude_x,magnitude_stddev = 0.15,augmentations_per_image=augmentations_per_image_x,rate = 1,geometric = True)(img)\n",
        "      aug_img_nump = aug_img.numpy().astype('int')\n",
        "      cv2.imwrite(trein_1235 + nome_classe + '/' + 't' + str(j) + '_' + i, aug_img_nump)\n",
        "\n",
        "  #partição 3\n",
        "  for i in trein_1245_names:    #manda a original\n",
        "    img = cv2.imread(trein_1245 + nome_classe + '/' + i)\n",
        "    for j in range(n_transformacoes):\n",
        "      aug_img = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=magnitude_x,magnitude_stddev = 0.15,augmentations_per_image=augmentations_per_image_x,rate = 1,geometric = True)(img)\n",
        "      aug_img_nump = aug_img.numpy().astype('int')\n",
        "      cv2.imwrite(trein_1245 + nome_classe + '/' + 't' + str(j) + '_' + i, aug_img_nump)\n",
        "\n",
        "  #partição 4\n",
        "  for i in trein_1345_names:    #manda a original\n",
        "    img = cv2.imread(trein_1345 + nome_classe + '/' + i)\n",
        "    for j in range(n_transformacoes):\n",
        "      aug_img = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=magnitude_x,magnitude_stddev = 0.15,augmentations_per_image=augmentations_per_image_x,rate = 1,geometric = True)(img)\n",
        "      aug_img_nump = aug_img.numpy().astype('int')\n",
        "      cv2.imwrite(trein_1345 + nome_classe + '/' + 't' + str(j) + '_' + i, aug_img_nump)\n",
        "\n",
        "  #partição 5\n",
        "  for i in trein_2345_names:    #manda a original\n",
        "    img = cv2.imread(trein_2345 + nome_classe + '/' + i)\n",
        "    for j in range(n_transformacoes):\n",
        "      aug_img = keras_cv.layers.RandAugment(value_range=(0, 255), magnitude=magnitude_x,magnitude_stddev = 0.15,augmentations_per_image=augmentations_per_image_x,rate = 1,geometric = True)(img)\n",
        "      aug_img_nump = aug_img.numpy().astype('int')\n",
        "      cv2.imwrite(trein_2345 + nome_classe + '/' + 't' + str(j) + '_' + i, aug_img_nump)\n",
        "\n",
        "  print(nome_classe)\n",
        "  print('Tamanho partição 1234 aumen:' + str(len(tf.io.gfile.listdir(trein_1234 + nome_classe))))\n",
        "  print('Tamanho partição 1235 aumen:' + str(len(tf.io.gfile.listdir(trein_1235 + nome_classe))))\n",
        "  print('Tamanho partição 1245 aumen:' + str(len(tf.io.gfile.listdir(trein_1245 + nome_classe))))\n",
        "  print('Tamanho partição 1345 aumen:' + str(len(tf.io.gfile.listdir(trein_1345 + nome_classe))))\n",
        "  print('Tamanho partição 2345 aumen:' + str(len(tf.io.gfile.listdir(trein_2345 + nome_classe))))\n"
      ],
      "metadata": {
        "id": "XLMKiJWeUvNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aumentar_imagens('covid', 0.2,3,2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MHFXEgkUH9E",
        "outputId": "0fa01b1b-dc1f-4582-8dcd-796aeec3ef32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "covid\n",
            "Tamanho partição 1234 aumen:1140\n",
            "Tamanho partição 1235 aumen:1140\n",
            "Tamanho partição 1245 aumen:1140\n",
            "Tamanho partição 1345 aumen:1140\n",
            "Tamanho partição 2345 aumen:1140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aumentar_imagens('dip', 0.2,3,4)"
      ],
      "metadata": {
        "id": "hbVwYFdhno2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcab1bb7-016e-48bc-bc1f-b4ed09311297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dip\n",
            "Tamanho partição 1234 aumen:1040\n",
            "Tamanho partição 1235 aumen:1050\n",
            "Tamanho partição 1245 aumen:1050\n",
            "Tamanho partição 1345 aumen:1050\n",
            "Tamanho partição 2345 aumen:1050\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aumentar_imagens('normais', 0.2,3,2)"
      ],
      "metadata": {
        "id": "sjyDhyJZno8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429f5c2c-3318-4c57-aa68-0170efd64405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "normais\n",
            "Tamanho partição 1234 aumen:1140\n",
            "Tamanho partição 1235 aumen:1140\n",
            "Tamanho partição 1245 aumen:1140\n",
            "Tamanho partição 1345 aumen:1140\n",
            "Tamanho partição 2345 aumen:1140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aumentar_imagens('tuberculose', 0.2,3,2)"
      ],
      "metadata": {
        "id": "ToGh4AnhUIKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2727ee-0bea-4096-90a2-dd86aec2ed32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tuberculose\n",
            "Tamanho partição 1234 aumen:1140\n",
            "Tamanho partição 1235 aumen:1140\n",
            "Tamanho partição 1245 aumen:1140\n",
            "Tamanho partição 1345 aumen:1140\n",
            "Tamanho partição 2345 aumen:1140\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}